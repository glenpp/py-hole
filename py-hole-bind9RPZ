#!/usr/bin/env python3
"""
Manages bind9 RPZ file (DNS Firewall) against configured blacklists
Copyright (C) 2017  Glen Pitt-Pladdy

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.


See: https://www.pitt-pladdy.com/blog/_20170407-105402_0100_DNS_Firewall_blackhole_malicious_like_Pi-hole_with_bind9/
"""


import time
import re
import os
import io
import sys
import argparse
import pwd
import grp
import json
import glob
import logging
import logging.config
import subprocess
import email.utils  # for date parsing
import requests
import yaml


USER_AGENT = 'py-hole RPZ blackhole manager'
# checks when wildcards are not allowed, and are allowed on a source
HOST_SANITIY_RE = re.compile(r'[\w\-\.]{3,253}')
HOST_SANITIY_WILDCARD_RE = re.compile(r'(?:\*\.)?[\w\-\.]{3,253}')
# overall checks
SANE_RECORD_MAX = 254
#SANE_LINE_MAX_DEFAULT = 256
SANE_LINE_RE_DEFAULT = r'[\w\-\+\*\.,:;# \t\(\)\[\]@&%/\'\\`"<>=!\?~\|\$]*'

CONFIG_DEFAULT = {
    # base config overridden by config_file
    'cachedir': '/var/local/bindRPZ',
    'cacheprefix': 'bindRPZcache-',
    'cache_sane_age_days': 14,  # after which this will fail
    'defaultresponse': 'CNAME .',
    'categories': {},
    'blacklists': {
        'StevenBlack': {
            'url': 'https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts',
            'format': 'hosts',
            'hostskey': '0.0.0.0',
            # sanity checks
            'sane_size_min': 1048576,
            'sane_size_max': 2097152,
            'sane_line_max': 300,
            # 'sane_line_re': r'',
            # 'cache_sane_age_days': 90,
            # and in post-processing:
            # 'ignore_line_exact': ['line must match this exactly after strip()'],
            # 'ignore_line_startswith': ['line must match start of line'],
        },
    },
    'exclusions': {},
}



class FatalErrorStop(Exception):
    """Custom immediate log and exit condition"""
    pass



# build our zone
class ZoneFile:
    """Render a bind RPZ zone file from a template
    """
    def __init__(self, config, category):
        """Constructor

        :arg config: dict, configuration data
        """
        self.config = config
        self.category = config['categories'][category]
        outputdata = self.category['rpz_template']
        outputdata = outputdata.replace(r'<SERIAL>', '{:010d}'.format(int(time.time())))
        self.output_lines = outputdata.splitlines(keepends=True)
        self.seen_before = {}   # keep track of what was seen before
        self.stats_totals = {}  # total hosts per source
        self.stats_skip = {}    # skip types per source
        self.excludes_used = {} # exclusions matched and source
        # work out length of origin / zone
        # bind seems to count maximum length with this appended
        template = self.config['categories'][category]['rpz_template']
        for line in template.splitlines():
            if line.startswith('$ORIGIN '):
                zone = line.split(' ', 1)[1]
                zone = zone.strip()
                self.zone_length = len(zone)
                break

    def add_comment(self, comment):
        """Add a comment line
        """
        self.output_lines.append(";{}\n".format(comment))

    def add_blank(self):
        """Add a blank line
        """
        self.output_lines.append("\n")

    def _stats_skip_increment(self, source, reason):
        if source not in self.stats_skip:
            self.stats_skip[source] = {}
        self.stats_skip[source][reason] = self.stats_skip[source].get(reason, 0) + 1


    def add_host(self, host, source, allow_wildcard=False):
        """Add a blacklisted host to bind zone file with safety checks

        :param host: str, host pattern to blacklist
        :param source: str, identifier for the source of this host pattern
        :param allow_wildcard: bool, optional (default False) allow wildcard host patterns
        :returns: tuple, consisting of:
            added: bool, True if the record was adeed, False if not (dup, bad record, etc.)
            reason: str|None, why the record was not added
        """
        # tidy host and sanity check
        host = host.lower().strip()
        sanity_re = HOST_SANITIY_WILDCARD_RE if allow_wildcard else HOST_SANITIY_RE
        if not sanity_re.fullmatch(host):
            if allow_wildcard:
                raise FatalErrorStop("bad wildcard host doesn't match format: {}".format(host))
            else:
                raise FatalErrorStop("bad host doesn't match format: {}".format(host))
        # keep track of stats
        self.stats_totals[source] = self.stats_totals.get(source, 0) + 1
        # exclude hosts we've already seen
        if host in self.seen_before:
            self.output_lines.append("; seenbefore in {} {}\n".format(self.seen_before[host], host))
            self._stats_skip_increment(source, "seenbefore")
        # exclude hosts we've explicitly set as excluded
        if host in self.config['exclusions']:
            self.output_lines.append("; excluded {}\n".format(host))
            self._stats_skip_increment(source, "excluded")
            # also keep track of excludes
            if host not in self.excludes_used:
                self.excludes_used[host] = {}
            self.excludes_used[host][source] = True
            return
        # try add the line
        # limit from https://www.ietf.org/rfc/rfc1035.txt
        outputline = "{} {}\n".format(host, self.config['defaultresponse'])
        total_length = len(host.rstrip('.')) + 1 + self.zone_length
        if total_length > SANE_RECORD_MAX:
            # TODO is it practical to turn this into a wildcard to retain safety?
            self.output_lines.append(';toolong in {} {}\n'.format(source, host))
            self._stats_skip_increment(source, "toolong")
            return
        self.output_lines.append(outputline)
        self.seen_before[host] = source

    def write(self):
        """Write out the bind RPZ sone file
        """
        temp_file = self.category['rpz_file'] + '.TMP'
        with open(temp_file, 'wt') as f_rpz:
            for line in self.output_lines:
                f_rpz.write(line)
        try:
            os.rename(self.category['rpz_file'], self.category['rpz_file'] + '.old')
        except FileNotFoundError:
            pass
        os.rename(temp_file, self.category['rpz_file'])

    def stats_source(self, source):
        """Report (log) per-source stats"""
        logging.info("Stats for %s", source)
        logging.info("+ Total hosts: %d", self.stats_totals[source])
        for reason in sorted(self.stats_skip.get(source, {})):
            logging.info("+ Skipped %s:  %0.1f%% (%d)",
                         reason,
                         100 * self.stats_skip[source][reason] / self.stats_totals[source],
                         self.stats_skip[source][reason])

    def stats_overall(self):
        """Report (log) overall stats"""
        logging.info("Stats overall (all sources)")
        total = sum(self.stats_totals.values())
        logging.info("  Total hosts: %d", total)
        total_skipped = {}
        for stats in self.stats_skip.values():
            for reason, count in stats.items():
                total_skipped[reason] = total_skipped.get(reason, 0) + count
        for reason in sorted(total_skipped):
            logging.info("  Skipped %s:  %0.1f%% (%d)",
                         reason,
                         100 * total_skipped[reason] / total,
                         total_skipped[reason])

    def stats_exclude(self):
        """Report (log) info about exclusions to enable better maintainance"""
        for host in sorted(self.config['exclusions']):
            if host == 'localhost':
                # special cases we add so ignore
                continue
            if host not in self.excludes_used:
                logging.warning("Unused exclusion: %s", host)



class FetchCache:
    """Safe fetching (HTTP GET) with caching
    """
    # between _safe_fetch_child() and safe_fetch()
    EXIT_SUCCESS = 0
    EXIT_UNKWNOWN = 1
    EXIT_CONNECT = 2
    EXIT_EXCEPT = 3
    EXIT_SANITY = 4

    def __init__(self, cache_dir, **kwargs):
        """Setup cachhe & http collector

        :kw user_agent: str, optional custom user agent string
        :kw fetch_user: str, optional custom user to perform http fetch as (defualt nobody)
        :kw fetch_group: str, optional custom group to perform http fetch as (defualt nogroup)
        :kw cache_time_min: int, optional mimimum seconds to cache a file for (default 1h)
        :kw cache_time_default: int, optional default seconds to cache a file for (default max of 12h or cache_time_min * 12)
        :kw cache_time_max: int, optional maximum seconds to cache a file for (default max of 7d or cache_time_default * 14)
        :kw cache_file_prefix: str, optional prefix to add to cache files, default ''
        :kw sane_line_max_default: int, optional default maximum line length (default 256)
        :kw sane_line_re_default: str, optional line sanity check regex (default quite permissive)
        :kw retry_attempts: int, optional number of retries for failed attempts (default 3)
        :kw retry_delay: int, optional seconds between retries for failed attempts (default 5)
        """
        self.cache_dir = cache_dir
        self.run_time = time.time()
        # clear out any None values which should be defualts
        kwargs_clean = {key: value for key, value in kwargs.items() if value is not None}
        # setup session
        self.session = requests.Session()
        self.session.headers = {
            'User-Agent': kwargs_clean.get('user_agent', 'Python HTTP Fetch & Cache'),
        }
        # setup configuration parameters
        self.fetch_user = kwargs_clean.get('fetch_user', 'nobody')
        self.fetch_group = kwargs_clean.get('fetch_group', 'nogroup')
        self.fetch_uid = pwd.getpwnam(self.fetch_user).pw_uid
        self.fetch_gid = grp.getgrnam(self.fetch_group).gr_gid
        self.cache_time_min = kwargs_clean.get('cache_time_min', 3600)
        self.cache_time_default = kwargs_clean.get('cache_time_default',
                                                   max(43200, self.cache_time_min * 12))
        if self.cache_time_default <= self.cache_time_min:
            raise ValueError("Invalid cache_time_default - should be more than self.cache_time_min")
        self.cache_time_max = kwargs_clean.get('cache_time_max',
                                               max(604800, self.cache_time_default * 14))
        if self.cache_time_max <= self.cache_time_default:
            raise ValueError("Invalid cache_time_max - should be more than self.cache_time_default")
        self.cache_file_prefix = kwargs_clean.get('cache_file_prefix', '')
        self.sane_line_max_default = kwargs_clean.get('sane_line_max_default', 256)
        self.sane_line_re_default = kwargs_clean.get(
            'sane_line_re_default',
            r'[\w\-\.:/]{{0,{}}}'.format(self.sane_line_max_default)
        )
        self.retry_attempts = kwargs.get('retry_attempts', 3)
        self.retry_delay = kwargs.get('retry_delay', 5)
        self.cache_fiels_expect = []
        # load current status
        self.cache_status_path = os.path.join(self.cache_dir,
                                              self.cache_file_prefix + 'cache_status.yaml')
        self.cache_status = {}
        try:
            with open(self.cache_status_path, 'rt') as f_status:
                self.cache_status = yaml.safe_load(f_status)
        except FileNotFoundError:
            pass


    def save_status(self):
        """Save the cache status into file
        """
        with open(self.cache_status_path + '.TMP', 'wt') as f_status:
            yaml.safe_dump(self.cache_status, f_status)
        os.rename(self.cache_status_path + '.TMP', self.cache_status_path)


    def path(self, url):
        """Calculate cache file path from url
        """
        filename = url
        filename = re.sub(r'^https?://', '', filename)
        filename = re.sub(r'[^\w\.\-]', '_', filename)
        path = os.path.join(self.cache_dir, self.cache_file_prefix + filename)
        self.cache_fiels_expect.append(path)
        return path


    def stray_cache_files(self):
        """Return a list of files in the cache that we haven't touched (leftovers/strays?)
        """
        stray_files = [
            path for path in glob.glob(os.path.join(self.cache_dir, self.cache_file_prefix + '*'))
            if path not in self.cache_fiels_expect and  path != self.cache_status_path
        ]
        return stray_files


    def _safe_fetch_child_logs(self, log_stream, status_code=None, headers=None):
        data = {
            'status_code': status_code,
            'headers': {},
            'logs': log_stream.getvalue(),
        }
        if headers is not None:
            data['headers'] = {key.lower(): value for key, value in headers.items()}
        sys.stdout.write(json.dumps(data))
        sys.stdout.write('\n')

    class _ChildFail(Exception):
        pass

    def _safe_fetch_url(self, url, fetch_config, head_only):
        """Fetch url retrying on appropriate errors

        :param url: str, url to fetch, see safe_fetch()
        :param fetch_config: dict, configuration specific to this fetch, see safe_fetch()
        :param head_only: bool, if we should perform HEAD rather than GET, see safe_fetch()
        :returns: requests response object
        """

        tls_verify = fetch_config.get('tls_verify', True)
        tries = self.retry_attempts
        response = None
        while response is None:
            tries -= 1
            try:
                if head_only:
                    response = self.session.head(url, verify=tls_verify)
                else:
                    response = self.session.get(url, verify=tls_verify)
                if tries < self.retry_attempts - 1:
                    logging.info("Succeeded with %d tries left fetching: %s", tries, url)
            except requests.exceptions.SSLError as exc:
                logging.exception("SSLError exception fetching %s", url)
                logging.error("Not retrying: %s", url)
                raise self._ChildFail(self.EXIT_EXCEPT)
            except (requests.exceptions.ConnectionError, requests.exceptions.Timeout) as exc:
                exception_name = exc.__class__.__name__
                if tries > 0:
                    logging.warning("%s (%d tries left) fetching %s :: %s",
                                    exception_name, tries, url, str(exc.args))
                    time.sleep(self.retry_delay)
                    continue
                # no tries left - fail
                logging.error("%s no more tries fetching %s :: %s",
                              exception_name, url, str(exc.args))
                raise self._ChildFail(self.EXIT_CONNECT)
        return response


    def _safe_fetch_child(self, url, fetch_config, head_only):
        """Safely (with minimum priveledge) fetch the url

        :param url: str, url to fetch, see safe_fetch()
        :param fetch_config: dict, configuration specific to this fetch, see safe_fetch()
        :param head_only: bool, if we should perform HEAD rather than GET, see safe_fetch()

        Internally, exit status used are as per constants above.
        """
        # remove log stream handlers - they break output sending stderr
        for handler in logging.getLogger().handlers:
            if not isinstance(handler, logging.StreamHandler):
                continue
            # remove this
            logging.getLogger().removeHandler(handler)
            break
        # add our own stream handler to capture for passing back to parent
        log_strings = io.StringIO()
        handler = logging.StreamHandler(stream=log_strings)
        formatter = logging.Formatter(fmt='safe_fetch[%(process)d]: %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        handler.setLevel(logging.DEBUG)
        logging.getLogger().addHandler(handler)
        # we must do everything in an exception catch to log problems
        try:
            # drop priveledge
            try:
                # these might not wrok if we don't have permission
                os.setgroups([])
                os.setgid(self.fetch_gid)
                os.setuid(self.fetch_uid)
            except PermissionError as exc:
                logging.warning("Can't drop privelege: %s", exc)
            os.umask(0o077)
            # fetch url
            response = None
            response = self._safe_fetch_url(url, fetch_config, head_only)
            # sanity checks
            header_length = None
            if 'Content-Length' in response.headers:
                header_length = int(response.headers['Content-Length'])
                if header_length > fetch_config['sane_size_max']:
                    logging.error("Content-Length header > sane_size_max: %s > %d",
                                  response.headers['Content-Length'],
                                  fetch_config['sane_size_max'])
                    raise self._ChildFail(self.EXIT_SANITY)
            if not head_only:
                # seems sometimes we get zero lenght for HEAD so min must be checked here
                if header_length is not None and not header_length:
                    logging.error("Zero Content-Length header: %s",
                                  response.headers['Content-Length'])
                    raise self._ChildFail(self.EXIT_SANITY)
                # sanity check data
                data_length = len(response.text.encode())
                if not data_length:
                    logging.error("Zero length data recieved")
                    raise self._ChildFail(self.EXIT_SANITY)
                # sanity check Content-Length != response.text
                if header_length is not None and data_length < header_length:
                    logging.error("Data length < Content-Length header %d < %d",
                                  data_length, header_length)
                    raise self._ChildFail(self.EXIT_SANITY)
                # sanity check sizes
                if data_length > fetch_config['sane_size_max']:
                    logging.error("Data length > sane_size_max: %d > %d",
                                  data_length,
                                  fetch_config['sane_size_max'])
                    raise self._ChildFail(self.EXIT_SANITY)
                if data_length < fetch_config['sane_size_min']:
                    logging.error("Data length < sane_size_min: %d < %d",
                                  data_length,
                                  fetch_config['sane_size_min'])
                    raise self._ChildFail(self.EXIT_SANITY)
            # apply sanity checks on response and output lines
            line_re = re.compile(fetch_config.get('sane_line_re', self.sane_line_re_default))
            line_max = fetch_config.get('sane_line_max', self.sane_line_max_default)
            for i, line in enumerate(response.text.splitlines(True)):
                if len(line) > line_max:
                    logging.error("Line %d length > sane_line_max: %d > %d",
                                  i + 1, len(line), line_max)
                    raise self._ChildFail(self.EXIT_SANITY)
                # take line before comment when sane_line_comment_ignore is enabled
                check_line = line.rstrip('\r\n')
                if 'sane_line_comment_ignore' in fetch_config:
                    check_line = check_line.split(fetch_config['sane_line_comment_ignore'])[0]
                    check_line = check_line.rstrip()
                if not line_re.fullmatch(check_line):
                    logging.error("Line %d no match for regex: %s", i + 1, check_line)
                    raise self._ChildFail(self.EXIT_SANITY)
            # format output to json metadata + body
            self._safe_fetch_child_logs(log_strings, response.status_code, response.headers)
            sys.stdout.write('\n')  # end of header
            for line in response.text.splitlines(True):
                sys.stdout.write(line.encode('unicode_escape').decode() + '\n')
        except self._ChildFail as exc:
            # first arg must be exit status
            if response:
                self._safe_fetch_child_logs(log_strings, response.status_code, response.headers)
            else:
                self._safe_fetch_child_logs(log_strings)
            sys.exit(exc.args[0])
        except:
            # unhandled exception - just log it
            logging.exception("Unhandled exception processing %s", url)
            if response:
                self._safe_fetch_child_logs(log_strings, response.status_code, response.headers)
            else:
                self._safe_fetch_child_logs(log_strings)
            sys.exit(self.EXIT_UNKWNOWN)
        sys.exit(self.EXIT_SUCCESS)


    def safe_fetch(self, url, config, head_only=False):
        """Safely (with minimum priveledge) fetch the url using _fetch_safe_child()


        :param url: str, URL to download if needed
        :parm config: dict, config relating to this url/fetch, with keys:
            sane_size_max: int, maximum reported size to accept
            sane_size_min: int, minimum reported size to accept
            sane_line_max': int, optional length to check lines against
                (default self.sane_line_max_default)
            sane_line_re: str, optional regex to check lines against
                (default self.sane_line_re_default)
            sane_line_comment_ignore: str, optional comment start character(s) after which
                will be ignored for sane_line_re checks
        :param head_only: bool, if we should perform HEAD rather than GET
        :returns: tuple, of:
            request_success: bool, if the http request was made
            metadata: dict, data about response (http code, headers, etc.)
            data: str, returned body
        """
        if 'sane_size_min' not in config:
            raise KeyError("Missing config 'sane_size_min' for: {}".format(url))
        if 'sane_size_max' not in config:
            raise KeyError("Missing config 'sane_size_max' for: {}".format(url))
        # fork and handl parent/child communication
        pid, child_fd = os.forkpty()
        if not pid:
            # child process - this safely fetches the data
            self._safe_fetch_child(url, config, head_only)
            return
        # parent process
        # read data coming back
        data = b''
        try:
            while True:
                data += os.read(child_fd, 1024)
        except OSError:
            pass
        data = data.decode('utf-8')
        # reap child
        _, status = os.waitpid(pid, 0)
        exit_status = status >> 8 & 0xff
        if exit_status == self.EXIT_UNKWNOWN:
            logging.error("Fetch unkown failure")
        elif exit_status == self.EXIT_CONNECT:
            logging.error("Fetch connection failed")
        elif exit_status == self.EXIT_EXCEPT:
            logging.error("Fetch http exception")
        elif exit_status == self.EXIT_SANITY:
            logging.error("Fetch sanity check failed")
        elif exit_status != self.EXIT_SUCCESS:
            logging.critical("Fetch failed with unknown exit_status | status: %d | %d",
                             exit_status, status)
        # split header from body
        header = None
        metadata = None
        if '\r\n\r\n' in data:
            header, data = data.split('\r\n\r\n', 1)
        elif exit_status != self.EXIT_SUCCESS:
            # something bad happened - assume all we got was a header
            header = data
            data = ''
        else:
            # problem - can't separate header - assume things went wrong
            logging.critical("child header could not be split - received lines follow:")
        if header is not None:
            # got a header - decode
            try:
                metadata = json.loads(header)
            except:
                logging.exception("Exception while decoding header")
        if metadata is not None:
            # good header - re-log any logs we have
                for log in metadata['logs'].splitlines():
                    parts = log.split(']: ', 1)
                    if len(parts) == 2:
                        pid, log = parts
                        _, pid = pid.split('[', 1)
                        level, log = log.split(' - ', 1)
                    else:
                        # another line in the last log
                        log = parts[0]
                    msg = "child[{}]: {}".format(pid, log)
                    if level not in ('DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'):
                        raise ValueError("Unhanled level: {}".format(level))
                    getattr(logging, level.lower())(msg)
        else:
            # something went wrong - log child process output
            if header is not None:
                # data we want made it to the header
                data = header
            for i, line in enumerate(data.splitlines()):
                if i >= 25:
                    logging.critical("child lines limited to %d", i)
                    break
                logging.critical("child[%d]: %s", pid, line)
            raise RuntimeError("Failed to process header from child process output.")
        # decode data from body
        lines = ''
        for line in data.splitlines():
            lines += line.encode().decode('unicode_escape')
        request_success = status == 0
        # done
        return request_success, metadata, lines


    def cached_fetch(self, url, config):
        """Fetch a URL with caching

        :param url: str, URL to download if needed
        :param config: dict, configuration specific to this fetch, see safe_fetch() and for caching:
            cache_time_max: int, optional maximum time to cache, overrides self.cache_time_max
            cache_time_min: int, optional minimum time to cache, overrides self.cache_time_min
        :returns: str, path of cached file that has been downloaded
        """
        cache_path = self.path(url)
        cache_info = self.cache_status.get(url, {})
        if not os.path.isfile(cache_path):
            # file is gone so force no status
            cache_info = {}
        # check expiry
        try:
            file_time = os.path.getmtime(cache_path)
        except FileNotFoundError:
            file_time = 0
        fetched_time = cache_info.get('fetched', file_time)
        fetched_age = self.run_time - fetched_time
        if fetched_age <= config.get('cache_time_min', self.cache_time_min):
            # force minimum caching time
            logging.debug("no expire due to minimum cache time")
            return cache_path
        expires = False
        if 'expires' in cache_info and self.run_time > cache_info['expires']:
            logging.debug("expire due to Expires header")
            expires = True
        elif fetched_age > config.get('cache_time_max', self.cache_time_max):
            logging.debug("expire due to configured maximum cache time")
            expires = True
        elif 'max_age_time' in cache_info and self.run_time > cache_info['max_age_time']:
            logging.debug("expire due to Cache-Control max-age")
            expires = True
        elif not any([key in cache_info for key in ['expires', 'cache_time_max', 'max_age_time']]) \
                and fetched_age > self.cache_time_default:
            # no mechanism for expiry - using default instead
            logging.debug("expire due to configured default cache time")
            expires = True
        if not expires and os.path.isfile(cache_path):
            logging.debug("no expire condition met")
            return cache_path

        # head first with files having Last-Modified header
        skip_fetch = False
        if 'last_modified' in cache_info or 'etag' in cache_info:
            # safely fetch as alow-privelege with sanity checks
            request_success, metadata, data = self.safe_fetch(url, config, head_only=True)
            if not request_success:
                return cache_path
            if metadata['status_code'] != 200:
                logging.error(
                    "Bad HTTP status fetching header %s: %d - %s",
                    url, metadata['status_code'],
                    data[:50] + ('...' if len(data) > 50 else '')
                )
                return cache_path
            if 'last-modified' in metadata['headers']:
                remote_file_time = email.utils.mktime_tz(email.utils.parsedate_tz(metadata['headers']['last-modified']))
                if remote_file_time <= file_time:
                    # not updated
                    logging.debug("no expire due to Last-Modified not being newer")
                    # update expires, max_age, max_age_time, and fetched still happen below anyway
                    skip_fetch = True
            if 'etag' in metadata['headers'] and 'etag' in cache_info:
                # potentially cache_info has -gzip appended within quotes so needs normalising
                header_etag_normalised = re.sub(r'-gzip("?)$', r'\1', metadata['headers']['etag'])
                cache_etag_normalised = re.sub(r'-gzip("?)$', r'\1', cache_info['etag'])
                if header_etag_normalised == cache_etag_normalised:
                    # no change
                    logging.debug("no expire due to ETag not changed")
                    # update expires, max_age, max_age_time, and fetched still happen below anyway
                    skip_fetch = True
        if skip_fetch and os.path.isfile(cache_path):
            # something shows it's the same
            logging.info("header check results in skip fetch")
        else:
            # need to download for real
            logging.info("fecthing (no or expired cache)")
            # safely fetch as alow-privelege with sanity checks
            request_success, metadata, data = self.safe_fetch(url, config)
            if not request_success:
                return cache_path
            if metadata['status_code'] != 200:
                logging.error(
                    "Bad HTTP status fetching header %s: %d - %s",
                    url, metadata['status_code'],
                    data[:50] + ('...' if len(data) > 50 else '')
                )
                return cache_path
        # process cache related headers
        cache_info = {
            'fetched': self.run_time,
        }
        if 'expires' in metadata['headers']:
            cache_info['expires'] = email.utils.mktime_tz(email.utils.parsedate_tz(metadata['headers']['expires']))
        if 'etag' in metadata['headers']:
            cache_info['etag'] = metadata['headers']['etag']
        if 'cache-control' in metadata['headers']:
            max_age = None
            for part in metadata['headers']['cache-control'].split(','):
                part = part.strip()
                if part.startswith('max-age='):
                    max_age = int(part.split('=')[1])
            if max_age is not None:
                cache_info['max_age'] = max_age
                cache_info['max_age_time'] = self.run_time + max_age
        remote_file_time = None
        if 'last-modified' in metadata['headers']:
            remote_file_time = email.utils.mktime_tz(email.utils.parsedate_tz(metadata['headers']['last-modified']))
            cache_info['last_modified'] = remote_file_time
        if not skip_fetch:
            logging.debug("Writing file: %s", cache_path)
            # store if we are for real
            temp_file = cache_path + '.TMP'
            with open(temp_file, 'wt') as f_raw:
                f_raw.write(data)
            # set file time from Last-Modified if available
            if remote_file_time is not None:
                os.utime(temp_file, times=(remote_file_time, remote_file_time))
            os.rename(temp_file, cache_path)
        # update status
        self.cache_status[url] = cache_info
        self.save_status()
        return cache_path










def main():
    # setup logger
    logging.config.dictConfig(
        {
            'version': 1,
            'formatters': {
                'basic': {'format': 'py-hole-bind9RPZ[%(process)d]: %(levelname)s - %(message)s'},
            },
            'handlers': {
                'console': {
                    'class': 'logging.StreamHandler',
                    'formatter': 'basic',
                    'level': 'DEBUG',
                    'stream': 'ext://sys.stderr',
                },
                'syslog': {
                    'class': 'logging.handlers.SysLogHandler',
                    'formatter': 'basic',
                    'level': 'INFO',
                    'address': '/dev/log',
                },
            },
            'loggers': {
                None: {
                    'handlers': ['syslog'] + (['console'] if sys.stderr.isatty() else []),
                    #'level': 'INFO',
                    'level': 'DEBUG',
                },
            },
        }
    )
    logging.info("begin updating blocklists...")

    # read command line arguments with argparse
    parser = argparse.ArgumentParser(
        description='A Pi-hole inspired DNS firewall for use with bind/named using RPZ'
    )
    parser.add_argument(
        '-c', '--config', type=str,
        help='Configuration file to use (default: /etc/bind/py-hole-bind9RPZ_config.yaml)',
        default='/etc/bind/py-hole-bind9RPZ_config.yaml'
    )
    args = parser.parse_args()


    try:
        # read config - load yaml file or error
        config = CONFIG_DEFAULT.copy()
        try:
            with open(args.config, 'rt') as f_conf:
                config.update(yaml.safe_load(f_conf))
            # always exclude localhost else we get it blocked for 127.0.0.1 keys
            config['exclusions']['localhost'] = True
        except FileNotFoundError:
            raise FatalErrorStop("Configuration file {} not found\n".format(args.config))
#        # at minimum we need to end up with an rpzfile
#        if 'rpzfile' not in config:
#            raise FatalErrorStop("Setting for 'rpzfile' not found in configuration {}\n".format(args.config))
#        # and a template with a serial number
#        if 'rpztemplate' not in config or not re.search(r'<SERIAL>', config['rpztemplate']):
#            raise FatalErrorStop("Setting for 'rpztemplate' including a serial number marker '<SERIAL>' not found in configuration {}\n".format(args.config))
#        # and a reloadzonecommand:
#        if 'reloadzonecommand' not in config:
#            raise FatalErrorStop("Setting for 'reloadzonecommand' not found in configuration {}\n".format(args.config))


        # sort out categories
        categories = config['categories'].copy()
        for category in categories:
            categories[category]['_zone'] = ZoneFile(config, category)


        cacher = FetchCache(
            config['cachedir'],
            user_agent=USER_AGENT,
            fetch_user=config.get('fetch_user'),
            fetch_group=config.get('fetch_group'),
            cache_time_min=config.get('cache_time_min'),
            cache_time_default=config.get('cache_time_default'),
            cache_time_max=config.get('cache_time_max'),
            # TODO from config
            # sane_line_max_default=SANE_LINE_MAX_DEFAULT,
            sane_line_re_default=SANE_LINE_RE_DEFAULT,
            cache_file_prefix=config['cacheprefix'],
        )


        # refresh cache as needed
        if not os.path.isdir(config['cachedir']):
            logging.info("Creating cache directory: %s", config['cachedir'])
            os.makedirs(config['cachedir'])
        time_now = time.time()
        if not isinstance(config['blacklists'], list):
            raise FatalErrorStop("Old style config file - please update before use")
        for info in config['blacklists']:
            logging.info("processing %s", info['url'])
            if info['format'] in ('hosts', 'raw'):
                info['sane_line_comment_ignore'] = '#'
            elif info['format'] == 'rpz':
                info['sane_line_comment_ignore'] = ';'
            else:
                raise FatalErrorStop("Unknown format {} for {}".format(info['format'], info['url']))
            if '_cachefile' in info:
                raise FatalErrorStop("Duplicate (file already calculated) source: {}".format(info['url']))
            info['_cachefile'] = cacher.cached_fetch(info['url'], info)

        # check we have sane files
        for info in config['blacklists']:
            try:
                cachefile_age = time_now - os.path.getmtime(info['_cachefile'])
                cachefile_age_days = cachefile_age / 86400
                if 'cache_sane_age' in config or 'cache_sane_age' in info:
                    logging.warning("cache_sane_age is deprecated, use cache_sane_age_days instead")
                if cachefile_age_days > info.get('cache_sane_age_days', config['cache_sane_age_days']):
                    raise FatalErrorStop(
                        "Cache file is too old (failing to fetch?) at {:0.1f} > {:0.1f} days: {}".format(
                            cachefile_age_days,
                            info.get('cache_sane_age_days', config['cache_sane_age_days']),
                            info['_cachefile'],
                        )
                    )
                if cachefile_age_days > config['cache_sane_age_days']:
                    logging.info("Cache file is older than normal (%0.1f days): %s",
                                 cachefile_age_days,
                                 info['_cachefile'])
            except FileNotFoundError:
                raise FatalErrorStop("Cache file is missing (never successfully fetched?): {}".format(info['_cachefile']))

        # process cached files
        for info in config['blacklists']:
            category = info.get('category', 'default')
            zone = categories[category]['_zone']
            if category not in categories:
                raise FatalErrorStop("Category '{}' is not defined".format(category))
            zone.add_blank()
            zone.add_comment("=============================================================================")
            zone.add_comment(" Source: {}".format(info['url']))
            zone.add_comment("=============================================================================")
            zone.add_blank()
            # process data
            recordcount = 0
            logging.info("processing cached file %s", info['_cachefile'])
            with open(info['_cachefile'], 'rt') as f_cache:
                if info['format'] == 'hosts':
                    # comments start "#", we only take lines matching "hostskey"
                    for line in f_cache:
                        line = line.rstrip('\r\n')
                        # ignore lines
                        if line in info.get('ignore_line_exact', []):
                            zone.add_comment("# IGNORE: " + line)
                            continue
                        elif 'ignore_line_startswith' in info:
                            ignore_match = False
                            for ignore in info['ignore_line_startswith']:
                                if line.startswith(ignore):
                                    ignore_match = True
                                    break
                            if ignore_match:
                                zone.add_comment("# IGNORE: " + line)
                                continue
                        # separate comments that could be whole line or after a record
                        match = re.fullmatch(r'([^#]*?)#(.*)', line)
                        if match:
                            line = match.group(1)
                            zone.add_comment(match.group(2).lstrip())
                        line = line.rstrip()
                        if not line:
                            continue
                        # process the host line
                        hostlist = re.split(r'\s+', line)
                        if len(hostlist) < 2:
                            raise RuntimeError("Bad line doesn't match format {}: {}".format(info['format'], line))
                        if hostlist[0] != info['hostskey']:
                            # not a matching key
                            continue
                        for host in hostlist[1:]:
                            if not HOST_SANITIY_RE.fullmatch(host):
                                raise FatalErrorStop("Bad line doesn't match format {}: {}".format(info['format'], line))
                            recordcount += 1
                            zone.add_host(host, info['url'])
                elif info['format'] == 'raw':
                    # comments start "#"
                    for line in f_cache:
                        line = line.rstrip('\r\n')
                        # ignore lines
                        if line in info.get('ignore_line_exact', []):
                            zone.add_comment("# IGNORE: " + line)
                            continue
                        elif 'ignore_line_startswith' in info:
                            ignore_match = False
                            for ignore in info['ignore_line_startswith']:
                                if line.startswith(ignore):
                                    ignore_match = True
                                    break
                            if ignore_match:
                                zone.add_comment("# IGNORE: " + line)
                                continue
                        # separate comments that could be whole line or after a record
                        match = re.fullmatch(r'([^#]*?)#(.*)', line)
                        if match:
                            line = match.group(1)
                            zone.add_comment(match.group(2).lstrip())
                        line = line.rstrip()
                        if not line:
                            continue
                        # process the raw (host only) line
                        host = line.strip()
                        if not HOST_SANITIY_RE.fullmatch(host):
                            raise FatalErrorStop("Bad line doesn't match format {}: {}".format(info['format'], line))
                        recordcount += 1
                        zone.add_host(host, info['url'])
                elif info['format'] == 'rpz':
                    # comments start ";", stirctly not starting at the beginning of line
                    for line in f_cache:
                        line = line.rstrip('\r\n')
                        # ignore lines
                        if line in info.get('ignore_line_exact', []):
                            zone.add_comment("# IGNORE: " + line)
                            continue
                        elif 'ignore_line_startswith' in info:
                            ignore_match = False
                            for ignore in info['ignore_line_startswith']:
                                if line.startswith(ignore):
                                    ignore_match = True
                                    break
                            if ignore_match:
                                zone.add_comment("# IGNORE: " + line)
                            continue
                        # separate comments that could be whole line or after a record
                        match = re.fullmatch(r'([^;]*?);(.*)', line)
                        if match:
                            line = match.group(1)
                            zone.add_comment(match.group(2).lstrip())
                        line = line.rstrip()
                        if not line:
                            continue
                        # handle zone header items - skip them
                        if line.startswith('$TTL '):
                            continue
                        if re.match(r'@?\s+SOA\s', line):
                            continue
                        if re.match(r'@?\s+NS\s', line):
                            continue
                        # process the zone lines
                        host, response = line.strip().split(None, 1)
                        if not re.fullmatch(r'CNAME\s+\.', response):
                            raise FatalErrorStop("rpz response not as expected: {}".format(line))
                        if not HOST_SANITIY_RE.fullmatch(host):
                            raise FatalErrorStop("Bad line doesn't match format {}: {}".format(info['format'], line))
                        recordcount += 1
                        zone.add_host(host, info['url'], allow_wildcard=True)
                else:
                    raise FatalErrorStop("Unknown format {} for {}".format(info['format'], info['url']))
            if recordcount == 0:
                raise FatalErrorStop("Got record count of {:d} for {}".format(recordcount, info['url']))
            # stats for this srouce
            zone.stats_source(info['url'])

        # if we have a local blacklist, add that also
        if 'localblacklist' in config:
            category = config.get('localblacklist_category', 'default')
            zone = categories[category]['_zone']
            if category not in categories:
                raise FatalErrorStop("Category '{}' is not defined".format(category))
            logging.info("adding localblacklist from %s", args.config)
            zone.add_blank()
            zone.add_comment("=============================================================================")
            zone.add_comment(" Source: Local blacklist from {}".format(args.config))
            zone.add_comment("=============================================================================")
            zone.add_blank()
            for host in config['localblacklist']:
                zone.add_host(host, args.config, allow_wildcard=True)
    except FatalErrorStop as exc:
        logging.error(exc.args[0])
        sys.exit(exc.args[0])
    except SystemExit:
        raise
    except:
        logging.exception("Unhandled exception:")
        raise

    # report on overall stats
    zone.stats_overall()
    zone.stats_exclude()
    # make some noise about stray cache files
    for path in cacher.stray_cache_files():
        logging.warning("stray cache file: %s", path)

    # write the rpz zone files
    for category in categories.values():
        category['_zone'].write()
    # reload bind zone file
    for category, category_info in categories.items():
        proc = subprocess.run(category_info['reload_zone_command'], capture_output=True)
        if proc.returncode:
            logging.error("reload_zone_command failed (exit %d) for category '%s': %s",
                          proc.returncode, category, proc.stderr)
        else:
            logging.debug("reload_zone_command successful for category '%s': %s",
                          category, proc.stdout)




if __name__ == '__main__':
    main()
